{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sergey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#датасет взят с\n",
    "#https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "top_words = 5000\n",
    "max_veview_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "def loss_plot(fig, ax, train_loss, test_loss, loss_name):\n",
    "    train_line = ax.plot(train_loss, color = 'black')\n",
    "    test_line = ax.plot(test_loss, color = 'red')\n",
    "    ax.set_xlabel('Эпоха')\n",
    "    ax.set_ylabel(loss_name)\n",
    "    ax.legend(('Тренировочная выборка', 'Тестовая выборка'))\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data=pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#взял немного кода с https://www.kaggle.com/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\n",
    "#для предобработки\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#подсчитаем кол-во слов за исключением стоп-слов\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(imdb_data['review'].str.lower().str.cat(sep = ' '))\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words_count = nltk.FreqDist(w for w in tokens if w not in stopwords) \n",
    "words_to_index = dict(zip([x[0] for x in words_count.most_common(top_words)], range(5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#переведём отзывы в списки индексов\n",
    "def to_list_indices(s):\n",
    "    return [words_to_index[x.lower()] for x in nltk.tokenize.word_tokenize(s) if x.lower() in words_to_index]\n",
    "\n",
    "imdb_data['indices'] = imdb_data['review'].apply(to_list_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#пофильтруем по длине отзывов\n",
    "data = imdb_data[imdb_data['indices'].apply(len) < max_veview_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#переводим метки в числа\n",
    "lb=LabelBinarizer()\n",
    "sentiment_data=lb.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#нарезаем выборки\n",
    "train_x, test_x = data['indices'][:int(len(data)*0.8)], data['indices'][int(len(data)*0.8):]\n",
    "train_y, test_y = sentiment_data[:int(len(data)*0.8)], sentiment_data[int(len(data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#дополняем короткие отзывы, переводим в тензоры\n",
    "pad_token = top_words\n",
    "train_x = nn.utils.rnn.pad_sequence([torch.Tensor(x).to(device).long() for x in train_x], padding_value = pad_token, batch_first = True)\n",
    "test_x = nn.utils.rnn.pad_sequence([torch.Tensor(x).to(device).long() for x in test_x], padding_value = pad_token, batch_first = True)\n",
    "test_y = torch.Tensor(test_y).to(device)\n",
    "train_y = torch.Tensor(train_y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#готовим загрузчик батчей\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, x, y):\n",
    "            self.y = y\n",
    "            self.x = x\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.x)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            X = self.x[index]\n",
    "            y = self.y[index]\n",
    "            return X, y\n",
    "    \n",
    "train_set = Dataset(train_x, train_y)\n",
    "test_set = Dataset(test_x, test_y)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 32)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=pad_token)\n",
    "        self.lstm = nn.LSTM(embed_dim, 100, 1, batch_first = True)\n",
    "        self.fc = nn.Linear(100, 1)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        x = self.drop(x)\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        #только последний выход lstm\n",
    "        x = x.view(-1, 100)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sig(x)\n",
    "        return x\n",
    "    \n",
    "model = TextSentiment(top_words, 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "cross_entropy = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274d419a1c2b434397e8a7f61b98f679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 train_loss:0.6934694310862979 test_loss:0.6932963947038273\n",
      "train_acc:0.49819809198379517 test_acc:0.4999887943267822\n",
      "epoch1 train_loss:0.6931414217243106 test_loss:0.6933853341427161\n",
      "train_acc:0.49819809198379517 test_acc:0.4961897134780884\n",
      "epoch2 train_loss:0.6929936456930387 test_loss:0.6930097763910716\n",
      "train_acc:0.49819809198379517 test_acc:0.5023534297943115\n",
      "epoch3 train_loss:0.6797938072459125 test_loss:0.6529885433909498\n",
      "train_acc:0.518546462059021 test_acc:0.6494531035423279\n",
      "epoch4 train_loss:0.6463825758590009 test_loss:0.637794469272618\n",
      "train_acc:0.5092588663101196 test_acc:0.6600546836853027\n",
      "epoch5 train_loss:0.6373666166772931 test_loss:0.6279243233598474\n",
      "train_acc:0.5731198787689209 test_acc:0.6748027801513672\n",
      "epoch6 train_loss:0.6270363118959752 test_loss:0.6197482044602329\n",
      "train_acc:0.5502104759216309 test_acc:0.6797224879264832\n",
      "epoch7 train_loss:0.6174012815757787 test_loss:0.6200338483273566\n",
      "train_acc:0.6130114197731018 test_acc:0.6711269617080688\n",
      "epoch8 train_loss:0.6055288728508916 test_loss:0.5887831976780524\n",
      "train_acc:0.6348913908004761 test_acc:0.6995697021484375\n",
      "epoch9 train_loss:0.5771438683315868 test_loss:0.5696082969645521\n",
      "train_acc:0.6628270149230957 test_acc:0.7110005617141724\n",
      "epoch10 train_loss:0.5558132892930424 test_loss:0.5389474883224025\n",
      "train_acc:0.6736443042755127 test_acc:0.7302312850952148\n",
      "epoch11 train_loss:0.5218150785891882 test_loss:0.5129048467238189\n",
      "train_acc:0.6928501725196838 test_acc:0.7610050439834595\n",
      "epoch12 train_loss:0.48870291128442006 test_loss:0.4737942489016028\n",
      "train_acc:0.7204464673995972 test_acc:0.7771763801574707\n",
      "epoch13 train_loss:0.46167863707442386 test_loss:0.483358617533337\n",
      "train_acc:0.7382683157920837 test_acc:0.7780504822731018\n",
      "epoch14 train_loss:0.4429344270022321 test_loss:0.44087327478371025\n",
      "train_acc:0.7525092959403992 test_acc:0.797718346118927\n",
      "epoch15 train_loss:0.42612231352598795 test_loss:0.4230248970446331\n",
      "train_acc:0.7661559581756592 test_acc:0.8099896907806396\n",
      "epoch16 train_loss:0.4117335186693774 test_loss:0.41554879843493997\n",
      "train_acc:0.778163731098175 test_acc:0.8066052794456482\n",
      "epoch17 train_loss:0.397807303512152 test_loss:0.40619913374002165\n",
      "train_acc:0.7876698970794678 test_acc:0.8185516595840454\n",
      "epoch18 train_loss:0.3852250724509879 test_loss:0.39739666570038784\n",
      "train_acc:0.7989855408668518 test_acc:0.8182938694953918\n",
      "epoch19 train_loss:0.3776806118987101 test_loss:0.40030011229026013\n",
      "train_acc:0.8062200546264648 test_acc:0.8220817446708679\n",
      "epoch20 train_loss:0.3691463773518731 test_loss:0.3893869796416143\n",
      "train_acc:0.8115511536598206 test_acc:0.8253989815711975\n",
      "epoch21 train_loss:0.3574032620863342 test_loss:0.3820374789007338\n",
      "train_acc:0.8172942399978638 test_acc:0.8254662156105042\n",
      "epoch22 train_loss:0.3502518468838194 test_loss:0.3814730445370252\n",
      "train_acc:0.8221134543418884 test_acc:0.831842839717865\n",
      "epoch23 train_loss:0.34014321439020284 test_loss:0.37264477682141434\n",
      "train_acc:0.8285121917724609 test_acc:0.8347902297973633\n",
      "epoch24 train_loss:0.3403504910012642 test_loss:0.3793414909835462\n",
      "train_acc:0.8298598527908325 test_acc:0.831260085105896\n",
      "epoch25 train_loss:0.33220731814355003 test_loss:0.378869714723392\n",
      "train_acc:0.8366573452949524 test_acc:0.8308230042457581\n",
      "epoch26 train_loss:0.3255831343849083 test_loss:0.3640910492545658\n",
      "train_acc:0.8386375308036804 test_acc:0.8430607914924622\n",
      "epoch27 train_loss:0.31546718437910637 test_loss:0.3751078021727798\n",
      "train_acc:0.8430925011634827 test_acc:0.833591103553772\n",
      "epoch28 train_loss:0.3160510538601792 test_loss:0.36082848220507857\n",
      "train_acc:0.843930184841156 test_acc:0.8389815092086792\n",
      "epoch29 train_loss:0.3109327857501996 test_loss:0.3736335681382315\n",
      "train_acc:0.8468324542045593 test_acc:0.8443719744682312\n",
      "epoch30 train_loss:0.3068980194263525 test_loss:0.36182335101606405\n",
      "train_acc:0.8506931662559509 test_acc:0.8446633219718933\n",
      "epoch31 train_loss:0.30127648531192247 test_loss:0.35893535549754585\n",
      "train_acc:0.8540439605712891 test_acc:0.8459745049476624\n",
      "epoch32 train_loss:0.297862281584309 test_loss:0.3689361722935816\n",
      "train_acc:0.8551980257034302 test_acc:0.8493253588676453\n",
      "epoch33 train_loss:0.29588041167124446 test_loss:0.35824139840848795\n",
      "train_acc:0.8570555448532104 test_acc:0.8442263007164001\n",
      "epoch34 train_loss:0.28977053753145926 test_loss:0.37740719879571444\n",
      "train_acc:0.8581596612930298 test_acc:0.8417496085166931\n",
      "epoch35 train_loss:0.28691281894946985 test_loss:0.3551887467781425\n",
      "train_acc:0.8606613278388977 test_acc:0.8481598496437073\n",
      "epoch36 train_loss:0.2835908880965276 test_loss:0.35469371346986933\n",
      "train_acc:0.8602721095085144 test_acc:0.8491460680961609\n",
      "epoch37 train_loss:0.27962689277105957 test_loss:0.3435471845499841\n",
      "train_acc:0.8659539222717285 test_acc:0.8474314212799072\n",
      "epoch38 train_loss:0.2748739985631896 test_loss:0.35610501675721096\n",
      "train_acc:0.8678728342056274 test_acc:0.8477227687835693\n",
      "epoch39 train_loss:0.2739418587606131 test_loss:0.36730682137823883\n",
      "train_acc:0.8677156567573547 test_acc:0.8495831489562988\n",
      "epoch40 train_loss:0.2719470096843226 test_loss:0.34553785127151265\n",
      "train_acc:0.8716607093811035 test_acc:0.8542787432670593\n",
      "epoch41 train_loss:0.2667853958678968 test_loss:0.35716941436895955\n",
      "train_acc:0.8722070455551147 test_acc:0.8542787432670593\n",
      "epoch42 train_loss:0.265578951934105 test_loss:0.3604972825878428\n",
      "train_acc:0.8745744824409485 test_acc:0.8545700907707214\n",
      "epoch43 train_loss:0.2596682524503945 test_loss:0.35472242691334865\n",
      "train_acc:0.8774154186248779 test_acc:0.859052836894989\n",
      "epoch44 train_loss:0.25997236550357933 test_loss:0.35672131125187817\n",
      "train_acc:0.8765048384666443 test_acc:0.8545700907707214\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure(figsize=(8,10))\n",
    "ax = fig.subplots(2,1)\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "ax1, ax2 = ax\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "epochs = 45\n",
    "for i in range(epochs):\n",
    "    train_loss_epoch = 0\n",
    "    train_acc_epoch = 0\n",
    "    test_loss_epoch = 0\n",
    "    test_acc_epoch = 0\n",
    "    for x, y in train_loader:\n",
    "        y_hat = model(x)\n",
    "        loss = cross_entropy(y_hat, y)\n",
    "        train_loss_epoch += loss.item()\n",
    "        train_acc_epoch += torch.mean((y == (y_hat > 0.5).float()).float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss.append(train_loss_epoch / len(train_loader))\n",
    "    train_acc.append(train_acc_epoch / len(train_loader))\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            y_hat = model(x)\n",
    "            loss = cross_entropy(y_hat, y)\n",
    "            test_loss_epoch += loss.item()\n",
    "            test_acc_epoch += torch.mean((y == (y_hat > 0.5).float()).float())\n",
    "    test_loss.append(test_loss_epoch / len(test_loader))\n",
    "    test_acc.append(test_acc_epoch / len(test_loader))\n",
    "    fig.suptitle('Архитектура с одной LSTM', fontsize=16)\n",
    "    loss_plot(fig,ax1,train_loss, test_loss, \"Кросс-энтропия\")\n",
    "    print(f'epoch{i} train_loss:{train_loss[-1]} test_loss:{test_loss[-1]}')\n",
    "    print(f'train_acc:{train_acc[-1]} test_acc:{test_acc[-1]}')\n",
    "    loss_plot(fig,ax2,train_acc, test_acc, \"Точность\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextConvSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=pad_token)\n",
    "        self.conv = nn.Conv1d(32, 32, 3, padding = 3 // 2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.lstm = nn.LSTM(32, 100, 1, batch_first = True)\n",
    "        self.fc = nn.Linear(100, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        #только последний выход lstm\n",
    "        x = x.view(-1, 100)\n",
    "        x = self.fc(x)\n",
    "        x = self.sig(x)\n",
    "        return x\n",
    "    \n",
    "model_conv = TextConvSentiment(top_words, 32).to(device)\n",
    "optimizer = torch.optim.Adam(model_conv.parameters(), lr = 0.0001)\n",
    "cross_entropy = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d04e3e3f62c4d43b4133f84e9761567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 train_loss:0.6932148238023123 test_loss:0.6931363497024927\n",
      "train_acc:0.5023482441902161 test_acc:0.5032275319099426\n",
      "epoch1 train_loss:0.6930073624724274 test_loss:0.6928830571941562\n",
      "train_acc:0.5064160227775574 test_acc:0.5061413049697876\n",
      "epoch2 train_loss:0.6722722182740698 test_loss:0.6203679828654914\n",
      "train_acc:0.5810214877128601 test_acc:0.6640891432762146\n",
      "epoch3 train_loss:0.5820476729553062 test_loss:0.5557728660551262\n",
      "train_acc:0.6965287923812866 test_acc:0.7128608822822571\n",
      "epoch4 train_loss:0.5243713517254327 test_loss:0.510332511195214\n",
      "train_acc:0.743125855922699 test_acc:0.7461897134780884\n",
      "epoch5 train_loss:0.4814610298905339 test_loss:0.4818242602137141\n",
      "train_acc:0.7706857323646545 test_acc:0.7683678865432739\n",
      "epoch6 train_loss:0.45092977467047307 test_loss:0.46538287543611373\n",
      "train_acc:0.7879133224487305 test_acc:0.7791487574577332\n",
      "epoch7 train_loss:0.42820677911484994 test_loss:0.45432201569730585\n",
      "train_acc:0.8010252118110657 test_acc:0.7893469333648682\n",
      "epoch8 train_loss:0.4093405230444072 test_loss:0.44595890572090524\n",
      "train_acc:0.8109319806098938 test_acc:0.7967770099639893\n",
      "epoch9 train_loss:0.39289238411577154 test_loss:0.43965884673220296\n",
      "train_acc:0.8204994201660156 test_acc:0.8019096255302429\n",
      "epoch10 train_loss:0.3782041075711067 test_loss:0.4348473492445368\n",
      "train_acc:0.8284757733345032 test_acc:0.8078828454017639\n",
      "epoch11 train_loss:0.3649022446506785 test_loss:0.431118403211896\n",
      "train_acc:0.8363429307937622 test_acc:0.812253475189209\n",
      "epoch12 train_loss:0.35266135367271784 test_loss:0.42812668332408915\n",
      "train_acc:0.8426074981689453 test_acc:0.8156042695045471\n",
      "epoch13 train_loss:0.3412950802377332 test_loss:0.42600634008090255\n",
      "train_acc:0.8487992286682129 test_acc:0.8185180425643921\n",
      "epoch14 train_loss:0.33060785498912937 test_loss:0.4245165849586467\n",
      "train_acc:0.8539347052574158 test_acc:0.8204119801521301\n",
      "epoch15 train_loss:0.32048985321673895 test_loss:0.42316199156579437\n",
      "train_acc:0.8592523336410522 test_acc:0.8234714269638062\n",
      "epoch16 train_loss:0.3109171431780834 test_loss:0.42292828824032436\n",
      "train_acc:0.8647884130477905 test_acc:0.8243455290794373\n",
      "epoch17 train_loss:0.30180604500578834 test_loss:0.4241813533094935\n",
      "train_acc:0.8686491847038269 test_acc:0.8271136283874512\n",
      "epoch18 train_loss:0.2931474015759282 test_loss:0.4263703624841633\n",
      "train_acc:0.8733725547790527 test_acc:0.8285704851150513\n",
      "epoch19 train_loss:0.28489580578094736 test_loss:0.4293090773785448\n",
      "train_acc:0.8771604299545288 test_acc:0.8294445872306824\n",
      "epoch20 train_loss:0.2770077888365392 test_loss:0.4330519453594179\n",
      "train_acc:0.8809118866920471 test_acc:0.8295902609825134\n",
      "epoch21 train_loss:0.2694540022370907 test_loss:0.43718453163242005\n",
      "train_acc:0.8844084143638611 test_acc:0.8311928510665894\n",
      "epoch22 train_loss:0.2620402359271244 test_loss:0.4419973642024752\n",
      "train_acc:0.8883419632911682 test_acc:0.8319212794303894\n",
      "epoch23 train_loss:0.25473957764970395 test_loss:0.44806444195548556\n",
      "train_acc:0.8913285732269287 test_acc:0.8323583602905273\n",
      "epoch24 train_loss:0.24786465300251415 test_loss:0.4563851905574193\n",
      "train_acc:0.8951528668403625 test_acc:0.8329411149024963\n",
      "epoch25 train_loss:0.24111694367146574 test_loss:0.4642123437827423\n",
      "train_acc:0.8989407420158386 test_acc:0.8332324624061584\n",
      "epoch26 train_loss:0.23549257891387235 test_loss:0.4768058463740043\n",
      "train_acc:0.9017087817192078 test_acc:0.8313385248184204\n",
      "epoch27 train_loss:0.2297234991630474 test_loss:0.4883376908964002\n",
      "train_acc:0.9046589732170105 test_acc:0.8327954411506653\n",
      "epoch28 train_loss:0.22461337250568372 test_loss:0.49307349844377635\n",
      "train_acc:0.9075727462768555 test_acc:0.8333781957626343\n",
      "epoch29 train_loss:0.21906260148315995 test_loss:0.49658797663964316\n",
      "train_acc:0.9107414484024048 test_acc:0.8332324624061584\n",
      "epoch30 train_loss:0.21841108331582693 test_loss:0.5190470916564232\n",
      "train_acc:0.9122711420059204 test_acc:0.8330532312393188\n",
      "epoch31 train_loss:0.21102562572563932 test_loss:0.5193123691878119\n",
      "train_acc:0.9156948328018188 test_acc:0.834073007106781\n",
      "epoch32 train_loss:0.2040077300529519 test_loss:0.5294932355075241\n",
      "train_acc:0.9203203916549683 test_acc:0.8336695432662964\n",
      "epoch33 train_loss:0.1996385921661184 test_loss:0.5298724520326286\n",
      "train_acc:0.9225785732269287 test_acc:0.8330867886543274\n",
      "epoch34 train_loss:0.19766138384166437 test_loss:0.5378914814103734\n",
      "train_acc:0.9238897562026978 test_acc:0.8325040340423584\n",
      "epoch35 train_loss:0.19107411684151435 test_loss:0.5317355691865607\n",
      "train_acc:0.9278233051300049 test_acc:0.8320669531822205\n",
      "epoch36 train_loss:0.18896658666049823 test_loss:0.5278904702892149\n",
      "train_acc:0.9282239675521851 test_acc:0.8327954411506653\n",
      "epoch37 train_loss:0.18863269002426478 test_loss:0.5285131444325258\n",
      "train_acc:0.9282239675521851 test_acc:0.8333781957626343\n",
      "epoch38 train_loss:0.1790706955356684 test_loss:0.5465099242188054\n",
      "train_acc:0.9343064427375793 test_acc:0.8323583602905273\n",
      "epoch39 train_loss:0.18211604517804228 test_loss:0.5589194331656803\n",
      "train_acc:0.9312469959259033 test_acc:0.8307222127914429\n",
      "epoch40 train_loss:0.1756852543858555 test_loss:0.5618043927701585\n",
      "train_acc:0.9354354739189148 test_acc:0.8299937844276428\n",
      "epoch41 train_loss:0.16777222761819205 test_loss:0.5425414433833081\n",
      "train_acc:0.9395147562026978 test_acc:0.8332324624061584\n",
      "epoch42 train_loss:0.1684227768844102 test_loss:0.5403428254801007\n",
      "train_acc:0.9391505122184753 test_acc:0.8345436453819275\n",
      "epoch43 train_loss:0.16457656336978668 test_loss:0.5837212227206755\n",
      "train_acc:0.9402796030044556 test_acc:0.8267886638641357\n",
      "epoch44 train_loss:0.1587118096845029 test_loss:0.5463136263009343\n",
      "train_acc:0.9435575604438782 test_acc:0.8323248028755188\n"
     ]
    }
   ],
   "source": [
    "#у меня тут быстро произошёл оверфитинг\n",
    "%matplotlib widget\n",
    "fig = plt.figure(figsize=(8,10))\n",
    "ax = fig.subplots(2,1)\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "ax1, ax2 = ax\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "epochs = 45\n",
    "for i in range(epochs):\n",
    "    train_loss_epoch = 0\n",
    "    train_acc_epoch = 0\n",
    "    test_loss_epoch = 0\n",
    "    test_acc_epoch = 0\n",
    "    for x, y in train_loader:\n",
    "        y_hat = model_conv(x)\n",
    "        loss = cross_entropy(y_hat, y)\n",
    "        train_loss_epoch += loss.item()\n",
    "        train_acc_epoch += torch.mean((y == (y_hat > 0.5).float()).float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss.append(train_loss_epoch / len(train_loader))\n",
    "    train_acc.append(train_acc_epoch / len(train_loader))\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            y_hat = model_conv(x)\n",
    "            loss = cross_entropy(y_hat, y)\n",
    "            test_loss_epoch += loss.item()\n",
    "            test_acc_epoch += torch.mean((y == (y_hat > 0.5).float()).float())\n",
    "    test_loss.append(test_loss_epoch / len(test_loader))\n",
    "    test_acc.append(test_acc_epoch / len(test_loader))\n",
    "    fig.suptitle('Архитектура с одной LSTM и свёрткой', fontsize=16)\n",
    "    loss_plot(fig,ax1,train_loss, test_loss, \"Кросс-энтропия\")\n",
    "    print(f'epoch{i} train_loss:{train_loss[-1]} test_loss:{test_loss[-1]}')\n",
    "    print(f'train_acc:{train_acc[-1]} test_acc:{test_acc[-1]}')\n",
    "    loss_plot(fig,ax2,train_acc, test_acc, \"Точность\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
